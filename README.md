# Web Scraper с графическим интерфейсом

Многофункциональный веб-скрапер с современным графическим интерфейсом, разработанный на Python с использованием CustomTkinter.

## Возможности

### Извлечение данных
- Поддержка извлечения различных типов контента:
  - Ссылки (с автоматическим исправлением относительных URL)
  - Заголовки (h1, h2, h3)
  - Текстовый контент
- Рекурсивный обход страниц с настраиваемой глубиной (до 5 уровней)
- Интеллектуальная фильтрация контента:
  - Минимальная длина текста
  - Исключение по регулярным выражениям

### Интерфейс
- Современный дизайн с поддержкой светлой и темной темы
- Отображение статистики в реальном времени
- Прогресс-бар для отслеживания процесса
- Сохранение настроек между сессиями

### Экспорт данных
- Поддержка множества форматов:
  - Excel (.xlsx)
  - CSV с поддержкой Unicode
  - JSON с форматированием
  - HTML со встроенными стилями
  - Markdown
  - Текстовый формат с таблицами
- Статистика в каждом экспортированном файле

### Технические особенности
- Многопоточная обработка для быстрого сбора данных
- Автоматическое логирование операций
- Обработка ошибок с информативными сообщениями
- Поддержка различных кодировок
- Автоматическое исправление URL

## Установка

1. Клонируйте репозиторий:
```bash
git clone https://github.com/yourusername/web-scraper.git
cd web-scraper
```

2. Установите зависимости:
```bash
pip install -r requirements.txt
```

## Использование

1. Запустите программу:
```bash
python web_scraper.py
```

2. Введите URL сайта для анализа
3. Настройте параметры:
   - Выберите глубину поиска
   - Отметьте типы данных для извлечения
   - Установите фильтры при необходимости
4. Нажмите "Начать извлечение"
5. После завершения экспортируйте результаты в нужном формате

## Настройка фильтров

### Минимальная длина текста
Позволяет отфильтровать короткие фрагменты текста. Установите значение в символах.

### Исключающие паттерны
Используйте регулярные выражения для фильтрации нежелательного контента. Примеры:
- `реклама|контакты` - исключит тексты со словами "реклама" или "контакты"
- `^\s*Copyright` - исключит тексты, начинающиеся с "Copyright"

## Логирование

Программа автоматически создает лог-файлы в директории `logs` с именем формата `scraper_YYYYMMDD.log`. Логи содержат информацию о:
- Запусках программы
- Обработанных URL
- Возникших ошибках
- Экспорте данных

## Сохранение настроек

Все настройки автоматически сохраняются в файл `settings.json` и включают:
- Последний использованный URL
- Выбранную тему оформления
- Глубину поиска
- Настройки фильтров
- Выбранные типы данных для извлечения

## Требования

- Python 3.8+
- Зависимости из requirements.txt

## Известные ограничения

- Некоторые сайты могут блокировать автоматический сбор данных
- JavaScript-генерируемый контент может быть недоступен
- При большой глубине обхода время выполнения может быть значительным

## Планы по развитию

- [ ] Поддержка JavaScript-рендеринга через Selenium
- [ ] Добавление прокси-серверов
- [ ] Расширенные фильтры для данных
- [ ] Сохранение результатов в базу данных
- [ ] Поддержка регулярных выражений для извлечения данных 